\section{Methods}
\label{sec:methods}

\paragraph{QDevice Model}

The QDevice includes a physical quantum device, which can initialize and store quantum bits (qubits) which are individually identified by a physical address, apply quantum gates, measure qubits, and create entanglement with QDevices on other nodes (either entangle-and-measure, or entangle-and-keep~\cite{dahlberg_2019_egp}). The QDevice exposes the following interface to QNodeOS (\cref{sec:appendix-qdevice}): number of qubits available, and the supported physical instructions that QNodeOS may send. Physical instructions include qubit initialization, single- and two-qubit gates, measurement, entanglement creation, and a `no-op' for do nothing. Each instruction has a corresponding response (including entanglement success or failure, or a measurement outcome) that the QDevice sends back to QNodeOS.

QNodeOS and the QDevice interact by passing messages back and forth on clock ticks at a fixed rate (100 kHz in our NV implementation, 50 kHz in the trapped-ion implementation). During each tick, at the same time (1) QNodeOS sends physical instruction to QDevice, (2) QDevice can send a response (for a previous instruction). Upon receiving an instruction, the QDevice performs the appropriate (sequence of) operations (e.g. a particular pulse sequence in the AWG). An instruction may take multiple ticks to complete, where the QDevice returns the response (success, fail, outcome) during the first clock tick following completion. The QDevice handles an entanglement instruction by performing (a batch of) entanglement generation attempts~\cite{pompili_2022_experimental} (synchronized by the QDevice with the neighboring node's QDevice). 

\paragraph{QNodeOS Architecture}

QNodeOS consists of two layers: CNPU and QNPU (\cref{fig:fig2}a, \cref{sec:architecture}, Supplementary). Processes on the QNPU are managed by the Process Manager, and executed by the local processor. Executing a user process means executing NetQASM~\cite{dahlberg_2022_netqasm} subroutines (quantum blocks) or that process, which involves running classical instructions (including flow control logic) on the QNPU's local processor, sending entanglement requests to the network stack, and handling local quantum operations by sending physical instructions to the QDriver (\cref{fig:fig2}a). Executing the network process means asking the network stack which request (if any) to handle and sending the appropriate (entanglement generation) instructions to the QDevice. 

A QNPU process can be in the following states (\cref{fig:process-states} in Supplementary for state diagram): idle, ready, running and waiting. A QNPU process is running when the QNPU processor is assigned to it. The network process becomes ready when a network schedule time-bin starts; it becomes waiting when it finished executing and waits for the next time-bin; it is never idle. A user process is ready when there is at least one NetQASM subroutine pending to be executed; it is idle otherwise; it goes into the waiting state when it requests entanglement from the network stack (using NetQASM entanglement instructions~\cite{dahlberg_2022_netqasm}) and is made ready again when the requested entangled qubit(s) are delivered. 

The QNPU scheduler oversees all processes (user and network) on the QNPU, and chooses which ready process is assigned to the QNPU processor. CNPU processes can run concurrently, and their execution (order) is handled by the CNPU scheduler. The QNPU scheduler operates independently and only acts on QNPU processes. CNPU processes can only communicate with their corresponding QNPU processes. Since multiple programs can run concurrently on QNodeOS, the QNPU may have multiple user processes that have subroutines waiting to be executed at the same time. This hence requires scheduling on the QNPU.

Processes allocate qubits through the Quantum Memory Management Unit (QMMU), which manages virtual qubit address spaces for each process, and translates virtual addresses to physical addresses in the QDevice. The QMMU can also transfer ownership of qubits between processes, for example from the network process (having just created an entangled qubit), to a user process that requested this entanglement. The Network Stack uses Entanglement Request (ER) sockets (opened by user programs through QNPU API once execution starts) to represent quantum connections with programs on other nodes. The Entanglement Management Unit (EMU) maintains all ER sockets and makes sure that entangled qubits are moved to the correct process.

\paragraph{NV QDevice Implementation}

The two-node network employed in this work includes the nodes “Bob” (server) and “Charlie” (client) (separated by 3 meters) described in~\cite{pompili_2021_multinode,hermans2022qubit,pompili_2022_experimental}. For the QDevice, we replicated the setup used by~\cite{pompili_2022_experimental}, which mainly consists of: an Adwin-Pro II~\cite{adwin} acting as the main orchestrator of the setup; a series of subordinate devices responsible for qubit control, including laser pulse generators, optical readout circuits and an arbitrary waveform generator (Zurich Instruments HDAWG~\cite{zurich_instruments_hdawg_2019}). The quantum physical device, based on NV centers, counts one qubit for each node. The two QDevices share a common 1 MHz clock for high-level communication and their AWGs are synchronized at sub-nanosecond level for entanglement attempts.

We address the challenge of limited memory lifetimes by employing dynamical decoupling (DD).  While waiting for further physical instructions to be issued, DD sequences are used to preserve the coherence of the electron spin qubit~\cite{de_lange_universal_2010}. DD sequences for NV-centers can prolong the coherence time ($T_{\text{coh}}$) up to hundreds of ms~\cite{hermans2022qubit} or even seconds~\cite{abobeih_2018_one_sec}. In our specific case, we measured $T_{\text{coh}}$=13(2) ms for the server node, corresponding to ~1300 DD pulses. The discrepancy to the state-of-the-art for similar setups is due to several factors. To achieve such long $T_{\text{coh}}$, a thorough investigation of the nuclear spin environment is necessary to avoid unwanted interactions during long DD sequences, resulting in an even more accurate choice of interpulse delay. Other noise sources include unwanted laser fields, the quality of microwave pulses and electrical noise along the microwave line.  

A specific challenge arises at the intersection of extending memory lifetimes using DD, and the need for interactivity: to realize individual physical instructions, many waveforms realizing are uploaded to the Arbitrary Waveform Generator (AWG), where the QDevice decodes instructions sent by QNodeOS into specific preloaded pulse sequences. This results in a waveform table, containing 170 entries. The efficiency of the waveforms is limited by the AWG's waveform granularity that corresponds to steps that are multiples of 6.66 ns, having a direct impact on the $T_{\text{coh}}$. We are able to partially overcome this limitation through the methods described in~\cite{corna_efficient_2021}. Namely, each preloaded waveform, corresponding to one single instruction, has to be uploaded 16 times in order to be executed with sample precision. To not fill up the waveform memory of the device, we apply the methods in~\cite{corna_efficient_2021} only to the DD pulses that are played while the QDevice waits for an instruction from the QNPU, whereas the instructed waveforms (gate/operation + first block of XY8 DD sequence) are padded according to the granularity, if necessary.
The physical instructions supported by our NV QDevice is given in~\cref{sec:qdevice-nv}.

\paragraph{NV QNPU Implementation}

The QNPUs for both nodes are implemented in C++ on top of FreeRTOS~\cite{freertos}, a real-time operating system for microcontrollers. The stack runs on a dedicated MicroZed~\cite{microzed}---an off-the-shelf platform based on the Zynq-7000 SoC, which hosts two ARM Cortex-A9 processing cores, of which only one is used, clocked at 667 MHz. The QNPU was implemented on top of FreeRTOS to avoid re-implementing standard OS primitives like threads and network communication. FreeRTOS provides basic OS abstractions like tasks, inter-task message passing, and the TCP/IP stack. The FreeRTOS kernel---like any other standard OS---cannot however directly manage the quantum resources (qubits, entanglement requests and entangled pairs), and hence its task scheduler cannot take decisions based on such resources. The QNPU scheduler adds these capabilities (\cref{sec:qnpu_impl_scheduler}).

The QNPU connects to peer QNPUs via TCP/IP over a Gigabit Ethernet interface (IEEE 802.3 over full-duplex Cat 5e). The communication goes via two network switches (Netgear JGS524PE, one per node). The two QNPUs are time-synchronized through their respective QDevices (granularity 10 $\mu$s), since these already are synchronized at the $\mu$s-level (common 1Mhz clock).

The QNPU interfaces with the QDevice's ADwin-Pro II through a 12.5 MHz SPI interface, used to exchange 4-byte control messages at a rate of 100 kHz.  

\paragraph{NV CNPU Implementation}

The CNPUs for both nodes are a Python runtime executing on a general-purpose desktop machine (4 Intel 3.20 GHz cores, 32 GB RAM, Ubuntu 18.04). The choice of using a high-level system was made as the communication between distant nodes would ultimately be in the ms-timescales, and this allows for ease of programming the application. The CNPU machine connects to the QNPU via TCP over a Gigabit Ethernet interface (IEEE 802.3 over full-duplex Cat 8, average ping RTT of 0.1 ms), via the same single network switch as mentioned above (one per node), and sends application registration requests and NetQASM subroutines over this interface (10 to 1000 bytes, depending on the length of the subroutine). CNPUs communicate with each other through the same two network switches.

\paragraph{Scheduler Implementation}

We use a single Linux process (Python) for executing programs on the CNPU. CNPU `processes' are realized as threads created within this single Python process. When running multiple programs concurrently, a pool of such threads is used. Scheduling of the Python process and its threads is handled by the Linux OS. Each thread establishes a TCP connection with the QNPU in order to use the QNPU API (including sending subroutines and receiving their results) and executes the classical blocks for its corresponding program. 
Both the CNPU and QNPU maintain processes for running programs. The CNPU scheduler (standard Linux scheduler, see above) schedules CNPU processes, which indirectly controls in which order subroutines from different programs arrive at the QNPU. The QNPU scheduler handles subroutines of the same process priority on a first-come-first-served (FCFS) basis, leading however to executions of QNPU processes not in the order submitted by the CNPU (\cref{sec:multitasking-scaling}).

Using only the CNPU scheduler is not sufficient since (1) we want to avoid millisecond delays needed to communicate scheduling instructions across CPNU and QNPU, (2) user processes need to be scheduled in conjunction with the network process (meeting the challenge of scheduling both local and network operations), which is only running on the QNPU, and (3) QNPU user processes need to be scheduled with respect to each other, (e.g. a user process is waiting after having requested entanglement, allowing another user process to be run; as observed in the multitasking demonstration). 

\paragraph{Sockets and the Network Schedule}
In an ER Socket, one node is a `creator' and the other a `receiver'. As long as an ER socket is open between the nodes, an entanglement request from only the creator suffices for the network stack to handle it in the next corresponding time-bin, i.e. the `receiver' can comply with entanglement generation even if no request has (yet) been made to its network stack.

\paragraph{Trapped-ion Implementation}

The experimental system used for the trapped-ion implementation is discussed in~\cite{teller2023integrating,teller2021heating} and is described in detail in~\cite{teller_measuring_2021}. The implementation itself is described in~\cite{fioretto_towards_2020}. We confine a single \CaPlus ion in a linear Paul trap; the trap is based on a 300 µm thick diamond wafer on which gold electrodes have been sputtered. The ion trap is integrated with an optical microcavity composed of two fiber-based mirrors, but the microcavity is not used here. The physical-layer control infrastructure consists of C++ software; Python scripts; a pulse sequencer that translates Python commands to a hardware description language for a field-programmable gate array (FPGA); and hardware that includes the FPGA, input triggers, direct digital synthesis (DDS) modules, and output logic.

QNodeOS provides physical instructions through a development FPGA board (Texas Instruments, LAUNCHXL2-RM57L75) that uses a serial peripheral interface (SPI). We programmed an additional board (Cypress, CY8CKIT-14376) that translates SPI messages into TTL signals compatible with the input triggers of our experimental hardware.
The implementation consisted of sequences composed of seven physical instructions: initialization, $R_x(\pi)$, $R_y(\pi)$, $R_x(\pi/2)$, $R_y(\pi/2)$, $R_y(-\pi/2)$, and measurement. First, we confirmed that message exchange occurred at the rate of 50 kHz as designed. Next, we confirmed that we could trigger the physical-layer hardware. Finally, we implemented seven different sequences. Each sequence was repeated $10^4$ times, which allowed us to acquire sufficient statistics to confirm that our QDriver results are consistent with operation in the absence of the higher layers of QNodeOS.

\paragraph{Metrics}

Both classical and quantum metrics are relevant in the performance evaluation: The quantum performance of our test programs is measured by the fidelity $F(\rho,\ket{\tau})$ of an experimentally obtained quantum state $\rho$ to a target state $\ket{\tau}$ where $F(\rho,\ket{\tau}) = \bra{\tau}\rho\ket{\tau}$, estimated by quantum tomography~\cite{paris_quantum_2004}. Classical performance metrics include device utilization $T_{\text{util}} = 1 - T_{\text{idle}} / T_{\text{total}}$ where $T_{\text{idle}}$ is the total time that the QDevice is not executing any physical instruction, and $T_{total}$ is the duration of the whole experiment excluding time spent on entanglement attempts (see below).

\paragraph{Experiment Procedure NV Demonstration}

Applications are written in Python using the NetQASM SDK~\cite{dahlberg_2022_netqasm} (code in~\cref{sec:app_source}), with a compiler targeting the NV flavour~\cite{dahlberg_2022_netqasm}, as it includes quantum instructions that can be easily mapped to the physical instructions supported by the NV QDevice. The client and server nodes independently start execution of their programs by invoking a Python script on their own CNPU, which then spawns the threads for each program. During application execution, the CNPUs have background processes running, including QDevice monitoring software.

A fixed network schedule is installed in the two QNPUs, with consecutive time-bins (all assigned to the client-server node pair) with a length of 10 ms (chosen to be equal to 1000 communication cycles between QNodeOS and QDevice as in Ref.~\cite{pompili_2022_experimental}) to assess the performance without introducing a dependence on a changing network schedule.  During execution, the CNPUs and QNPUs record events including their timestamps. After execution, corrections are applied to the results (see below) and event traces are used to compute latencies.

\paragraph{Delegated Quantum Computation}

Our demonstration of DQC (\cref{fig:fig3}) implements the effective single-qubit computation $\ket{\psi} = H \circ R_z(\alpha) \circ \ket{+}$ on the server, as a simple form of blind quantum computing (BQC) that hides the rotation angle $\alpha$ from the server, when executed with randomly chosen $\theta$, and not performing tomography. The remote entanglement protocol utilized is the single-photon protocol~\cite{cabrillo1999creation,bose1999proposal,hermans2023entangling} (\cref{sec:qdevice-nv}).

\paragraph{Filtering}

Results, with no post-selection, are presented including known errors that occur during the tomography single-shot readout (SSRO) process (\cref{fig:fig3}b, blue) (details on the correction Supplementary of~\cite{pompili_2021_multinode}). We also report the post-selected results in which data are filtered based on the outcome of the Charge-Resonance check~\cite{robledo2010control} after one application iteration (\cref{fig:fig3}b, purple). This filter enables the elimination of false events, specifically when the emitter of one of the two nodes is not in the right charge state (ionization) or the optical resonances are not correctly addressed by the laser fields after the execution of one iteration of DQC.

Additional filtering (\cref{fig:fig3}b latency filter) is done on those iterations that showed latency not compatible with the combination of $T_{\text{coh}}$ of the server and the average entangled state fidelity. For this filter, a simulation (using a depolarizing model, based on the measured value $T_{\text{coh}}$, \cref{sec:dqc-simulation}) was used to estimate the single qubit fidelity (given the entanglement fidelity measured above) as a function of the duration the server qubit stays live in memory in a single execution of the DQC circuit (\cref{fig:fig3}a). This gives a conservative upper bound of the duration as 8.95 ms, to obtain a fidelity of at least 0.667. All measurement results corresponding to circuit executions exceeding 8.95 ms duration were discarded (146 out of 7200 data points). 

Other main sources of infidelity, that are not considered in this analysis of the outcome, include, for instance, the non-zero probability of double excitation for the NV center~\cite{hermans2023entangling}. During entanglement generation, the NV center can be re-excited, leading to the emission of two photons that lower the heralded entanglement fidelity. The error can be corrected by discarding those events that registered, in the entanglement time-window, a photon at the heralding station (resonant Zero-Phonon Line photon) and another one locally at the node (off-resonant Phonon-Side Band photon). 

Finally, the dataset presented in~\cref{fig:fig3}b (not shown chronologically) was taken in “one shot” to prove the robustness of the physical layer, therefore no calibration of relevant experimental parameters was performed in between, leading to possible degradation of the overall performance of the NV-based setup.

The single qubit fidelity is calculated with the same methods as in~\cite{iuliano2024qubit}, measuring in the state $\ket{i}$ and in its orthogonal state $\ket{-i}$, provided that we expect the outcome $\ket{i}$, whereas the two-qubit state fidelity is computed taking into account only the same positive-basis correlators (XX, YY, ZZ).

\paragraph{Multitasking: Delegated Computation and Local Gate Tomography}

In the first multitasking evaluation, we concurrently execute two programs on the client: a DQC-client program (interacting with a DQC-server program on the server) and a Local Gate Tomography (LGT) program (on the client only) (\cref{fig:fig4}). The client CNPU runtime executes the threads executing the two different programs concurrently. The client QNPU has two active user processes, each continuously receiving new subroutines from the CNPU, which are scheduled with respect to each other and the network process.

Estimates of the fidelity (\cref{fig:fig4}b) include same corrections as in the Supplementary of~\cite{pompili_2021_multinode} To assess the quantum performance of the LGT application, we used a mocked entanglement generation process on the QDevices (executing entanglement actions without entanglement) to simplify the test: weak-coherent pulses on resonance with the NV transitions, that follow the regular optical path, are employed to trigger the CPLD in the entanglement heralding time-window. This results in comparable application behavior for DQC (comparable rates and latencies, \cref{sec:mocked_entanglement}) with respect to multitasking on QNodeOS.

\paragraph{Multitasking: QDevice Utilization when scaling number of programs}

We scale the number of programs being multitasked (\cref{fig:fig4}d): We observe how the client QNPU scheduler chooses the execution order of the subroutines submitted by the CNPU. DQC subroutines each have an entanglement instruction, causing the corresponding user process to go into the waiting state when executed (waiting for entanglement from the network process). The QNPU scheduler schedules another process [(56\%, 81\%, 99\%) for (N=1, N=2, N>2)] of the times that a DQC process is put into the waiting state (demonstrating that the QNPU schedules independently from the order in which the CNPU submits subroutines). The number of consecutive LGT subroutines (of any LGT process; LGT block execution time ~2.4 ms) that is executed in between DQC subroutines is 0.83 for N=1, increasing for each higher N until 1.65 for N = 5, showing that indeed idle times during DQC are partially filled by LGT blocks (\cref{sec:multitasking-scaling}).

Device utilization (see Metrics above) quantifies only the utilization factor in between entanglement generation time windows to fairly compare the multitasking and the non-multitasking scenario. In both scenarios, the same entanglement generation processes are performed, which hence have the same probabilistic durations in both cases. To avoid inaccurate results due to this probabilistic nature, we exclude the entanglement generation time windows in both cases.